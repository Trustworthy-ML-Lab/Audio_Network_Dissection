{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### currently only support AST\n",
    "### before running this code, please make sure you have run fig3_adjective_distribution.ipynb to get the processed output of open-concept identification module.\n",
    "import os\n",
    "os.chdir('..')\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from transformers import AutoProcessor, AutoModelForAudioClassification\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict, Counter\n",
    "from datasets import Audio, load_dataset\n",
    "import torch.nn.utils.prune as prune\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define some used function/class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "\tdata = [a['array'] for a in example['audio']]\n",
    "\treturn processor(data)\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "\tdef __init__(self, dataset, processor, mode='train', val_fold=1):\n",
    "\t\tself.data = []\n",
    "\t\tself.sampling_rate = 16000\n",
    "\n",
    "\t\tif mode == 'train':\n",
    "\t\t\tdataset = dataset.filter(lambda x: x['fold'] != val_fold)\n",
    "\t\telse:\n",
    "\t\t\tdataset = dataset.filter(lambda x: x['fold'] == val_fold)\n",
    "\n",
    "\t\tdataset = dataset.cast_column('audio', Audio(sampling_rate=self.sampling_rate))\n",
    "\t\tdataset = dataset.map(tokenization, batched=True)\n",
    "\n",
    "\t\tfor line in dataset['train']:\n",
    "\t\t\twav = torch.tensor(line['audio']['array']).to(torch.float32)\n",
    "\t\t\tprocessed_audio = processor(wav, sampling_rate = self.sampling_rate, return_tensor='pt')\n",
    "\t\t\tprocessed_audio = np.squeeze(np.array(processed_audio['input_values']))\n",
    "\t\t\tobj = [processed_audio, line['target'], line['filename']]\n",
    "\t\t\tself.data.append(obj)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.data[idx]\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "\t'''\n",
    "\tTake a list of samples from a Dataset and collate them into a batch.\n",
    "\tReturns:\n",
    "\t\tA dictionary of tensors\n",
    "\t'''\n",
    "\tinput_values = pad_sequence(torch.tensor(np.array([example[0] for example in batch])), batch_first=True, padding_value=0)\n",
    "\tlabels = torch.stack([torch.tensor(example[1], dtype=torch.long) for example in batch])\n",
    "\tfilenames = [d[-1] for d in batch]\n",
    "\treturn {'input_values': input_values, 'labels': labels, 'filenames': filenames}\n",
    "\n",
    "\n",
    "def compar_func(x, task):\n",
    "\tif task in ['preps', 'verbs', 'nouns', 'caption_len', 'adj_after_rbf_llmf']:\n",
    "\t\ty = x[task]\n",
    "\telif task == 'basic_adj':\n",
    "\t\ty = set(x['adj_after_rbf_llmf']).intersection(set(basic_adjs))\n",
    "\telif task == 'high_adj':\n",
    "\t\ty = set(x['adj_after_rbf_llmf']).difference(set(basic_adjs))\n",
    "\t# elif task in ['clear', 'high-pitched', 'high', 'loud']:\n",
    "\t# \ty = set(x['adj_after_rbf_llmf']).intersection(set([task]))\n",
    "\telse:\n",
    "\t\traise Exception('task not implemented')\n",
    "\tif type(y) == int:\n",
    "\t\treturn y\n",
    "\treturn len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter setup. exp is the parameter to generate fig4a~4c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_json = 'summaries/calibration_ast-esc50_esc50_esc50_top5_processed.json'\n",
    "max_pruned_nums = [6442, 5535, 4428, 3321, 2214, 1107] # 12% ~ 2% of all the 55346 neurons in AST\n",
    "input_dimension = {'attention_output': 768, 'intermediate': 768, 'output': 3072}\n",
    "processor = AutoProcessor.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\n",
    "dataset = load_dataset('ashraq/esc50')\n",
    "network_class_file = 'data/network_class/esc50.txt'\n",
    "pos = ['random', 'preps', 'verbs', 'nouns', 'caption_len', 'adj_after_rbf_llmf'] # exp1\n",
    "highlowadj = ['random', 'basic_adj', 'high_adj'] # exp2\n",
    "basic_adjs = ['random', 'clear', 'high-pitched', 'high', 'loud'] # exp3. 'high' is 'high-quality'.\n",
    "seeds = [20, 202, 2024]\n",
    "exp = 'fig4c'\n",
    "\n",
    "if exp == 'fig4a':\n",
    "\ttasks = pos # the only parameter\n",
    "elif exp == 'fig4b':\n",
    "    tasks = highlowadj\n",
    "elif exp == 'fig4c':\n",
    "    tasks = basic_adjs\n",
    "else:\n",
    "    raise Exception('exp not implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2class = {-1: None}\n",
    "with open(network_class_file) as f:\n",
    "\tlabel_neuron_match = f.readlines() \n",
    "\tlabel_neuron_match = [line.replace('\\n', '').split('\\t') for line in label_neuron_match]\n",
    "\tfor line in label_neuron_match:\n",
    "\t\tlabel2class[int(line[1])] = line[0]\n",
    "\n",
    "with open(in_json, 'r') as f:\n",
    "\tdata = json.load(f)\n",
    "\tori_data = list(data.items())\n",
    "\n",
    "if (os.path.exists('dev_dataset.pickle')):\n",
    "\tprint('cache exist, load cache')\n",
    "\twith open('dev_dataset.pickle', 'rb') as f:\n",
    "\t\tdev_dataset = pickle.load(f)\n",
    "else:\n",
    "\tprint('cache not exist')\n",
    "\tdev_dataset = AudioDataset(dataset, processor, mode='dev')\n",
    "\twith open('dev_dataset.pickle', 'wb') as f:\n",
    "\t\tpickle.dump(dev_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conduct neuron ablation and record the dropped accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for task in tqdm(tasks):\n",
    "\tresults[task] = {}\n",
    "\tfor max_pruned_num in max_pruned_nums:\n",
    "\t\tresults[task][max_pruned_num] = defaultdict(list)\n",
    "\t\tfor seed in seeds:\n",
    "\t\t\tdata = deepcopy(ori_data)\n",
    "\t\t\trandom.Random(seed).shuffle(data)\n",
    "\t\t\tdata = dict(data)\n",
    "\n",
    "\t\t\tif task == 'random' or task in basic_adjs:\n",
    "\t\t\t\tsorted_data = data\n",
    "\t\t\telse:\n",
    "\t\t\t\tfor k in data.keys():\n",
    "\t\t\t\t\tdata[k].update({'att': compar_func(data[k], task)})\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tsorted_data = sorted(data.items(), key=lambda x: x[1]['att'], reverse=True)\n",
    "\t\t\t\tsorted_data = dict(sorted_data)\n",
    "\n",
    "\t\t\tdev_loader = DataLoader(dev_dataset, batch_size=64, shuffle=False, pin_memory=True, collate_fn=collate_batch)\n",
    "\t\t\tmask_cnt = 0\n",
    "\t\t\tmasked_neuron = defaultdict(list)\n",
    "\t\t\tmasked_neuron_bias = defaultdict(list)\n",
    "\t\t\tmodel = AutoModelForAudioClassification.from_pretrained('Evan-Lin/ast-esc50', num_labels=50, ignore_mismatched_sizes=False)\n",
    "\t\t\tmodel = model.to('cuda')\n",
    "\t\t\tfor key, item in tqdm(sorted_data.items()):\n",
    "\t\t\t\tlayer = key.split('#')[0]\n",
    "\t\t\t\tneuron_id = key.split('#')[1]\n",
    "\n",
    "\t\t\t\tif layer == 'fc':\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\tlayer = layer.split('_')\n",
    "\t\t\t\tlayer_num, layer_name = layer[0], layer[1]\n",
    "\t\t\t\tif len(layer) == 3:\n",
    "\t\t\t\t\tlayer_name = layer[1] + '_' + layer[2]\n",
    "\t\t\t\t\n",
    "\t\t\t\tlayer_id = layer_num.strip('layer')\n",
    "\t\t\t\tdim = input_dimension[layer_name.strip(str(layer_id))]\n",
    "\t\t\t\tneuron_adjs = item['adj_after_rbf_llmf']\n",
    "\t\n",
    "\t\t\t\tif ((task in pos + highlowadj) or (task == 'random') or (task in basic_adjs and task in item['adj_after_rbf_llmf'])) and mask_cnt < max_pruned_num:\n",
    "\t\t\t\t\tmask_cnt += 1\n",
    "\t\t\t\t\tmasked_neuron[layer_num + '_' + layer_name].append([0 for _ in range(dim)])\n",
    "\t\t\t\t\tmasked_neuron_bias[layer_num + '_' + layer_name].append(0)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tmasked_neuron[layer_num + '_' + layer_name].append([1 for _ in range(dim)])\n",
    "\t\t\t\t\tmasked_neuron_bias[layer_num + '_' + layer_name].append(1)\n",
    "\t\t\t\n",
    "\t\t\tfor key, mask in masked_neuron.items():\n",
    "\t\t\t\tlayer_id = int(key.split('_')[0].strip('layer'))\n",
    "\t\t\t\tlayer_name = key.split('_')[1]\n",
    "\t\t\t\tif layer_name == 'attention':\n",
    "\t\t\t\t\tmodule = model.audio_spectrogram_transformer.encoder.layer[layer_id].attention.output.dense\n",
    "\t\t\t\telif layer_name == 'intermediate':\n",
    "\t\t\t\t\tmodule = model.audio_spectrogram_transformer.encoder.layer[layer_id].intermediate.dense\n",
    "\t\t\t\telif layer_name == 'output':\n",
    "\t\t\t\t\tmodule = model.audio_spectrogram_transformer.encoder.layer[layer_id].output.dense\n",
    "\t\t\t\tweight_mask = torch.tensor(mask).to('cuda')\n",
    "\t\t\t\tbias_mask = torch.tensor(masked_neuron_bias[key]).to('cuda')\n",
    "\n",
    "\t\t\t\tprune.custom_from_mask(module, 'weight', mask=weight_mask)\n",
    "\t\t\t\tprune.custom_from_mask(module, 'bias', mask=bias_mask)\n",
    "\t\t\t\n",
    "\t\t\tcorrect, total = 0, 0\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tfor batch in tqdm(dev_loader):\n",
    "\t\t\t\t\tbatch['input_values'] = batch['input_values'].to('cuda')\n",
    "\t\t\t\t\tbatch['labels'] = batch['labels'].to('cuda')\n",
    "\t\t\t\t\toutputs = model(batch['input_values'])\n",
    "\t\t\t\t\toutputs = outputs.logits\n",
    "\t\t\t\t\toutputs_list = outputs.detach().cpu().tolist()\n",
    "\t\t\t\t\toutputs = torch.argmax(outputs, dim = -1)\n",
    "\t\t\t\t\tlabels = batch['labels']\n",
    "\t\t\t\t\tcorrect += torch.sum(outputs == labels).detach().cpu().item()\n",
    "\t\t\t\t\ttotal += outputs.shape[0]\n",
    "\n",
    "\t\t\tresults[task][max_pruned_num]['correct'].append(correct)\n",
    "\t\t\tresults[task][max_pruned_num]['total'].append(total)\n",
    "\n",
    "\t\tassert all(x == results[task][max_pruned_num]['total'][0] for x in results[task][max_pruned_num]['total']), print('not all the same')\n",
    "\t\tavg_acc = sum(results[task][max_pruned_num]['correct']) / len(results[task][max_pruned_num]['correct'])\n",
    "\t\tprint(f'{task}_{max_pruned_num} performance: ', avg_acc / results[task][max_pruned_num]['total'][0])\n",
    "\t\tprint(f'{task}_{max_pruned_num} performance drop: ', 0.95 - avg_acc / results[task][max_pruned_num]['total'][0])\n",
    "\t\tresults[task][max_pruned_num] = avg_acc / results[task][max_pruned_num]['total'][0]\n",
    "\tresults[task][0] = 0.95\n",
    "\tresults[task] = dict(sorted(results[task].items()))\n",
    "print('results: ', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(0, 14, 2))\n",
    "markers = ['^', 'o', 's', '*', 'x', 'D']\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "for i, task in enumerate(tasks):\n",
    "\tplt.plot(x, list(results[task].values()), label=task, marker=markers[i])\n",
    "\n",
    "plt.ylabel('classification accuracy', fontsize=20)\n",
    "plt.xlabel('% of neurons pruned', fontsize=20)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'{exp}.jpg', format='jpg', dpi=1000) # high is high-quality"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
