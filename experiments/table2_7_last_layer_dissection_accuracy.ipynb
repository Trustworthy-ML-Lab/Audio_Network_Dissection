{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/yuxiang1234/sandbox-AND\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang1234/env/dissect/lib/python3.8/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yuxiang1234/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# from vllm import LLM, SamplingParams\n",
    "\n",
    "import similarity\n",
    "from sentence_utils import (get_basename, post_process_prediction)\n",
    "from data_utils import (save_discriminative_sample, get_clip_prediction, get_cls_id_to_label,\n",
    "                        get_concept_set, get_topk_acc, get_concept_dataset,\n",
    "get_audio_description, get_similarity_from_descriptions, read_json)\n",
    "from utils import save_activations, get_similarity_from_activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.target_name = \"ast-esc50\" # Model to dissect (target model)\n",
    "        \n",
    "        self.target_layers = \"fc\"\n",
    "        # self.target_layers = \"layer0_output,layer0_intermediate,layer0_attention_output, \\\n",
    "        #                 \t  layer1_output,layer1_intermediate,layer1_attention_output, \\\n",
    "        #                       layer2_output,layer2_intermediate,layer2_attention_output, \\\n",
    "        #                       layer3_output,layer3_intermediate,layer3_attention_output, \\\n",
    "        #                       layer4_output,layer4_intermediate,layer4_attention_output, \\\n",
    "        #                       layer5_output,layer5_intermediate,layer5_attention_output, \\\n",
    "        #                       layer6_output,layer6_intermediate,layer6_attention_output, \\\n",
    "        #                       layer7_output,layer7_intermediate,layer7_attention_output, \\\n",
    "        #                       layer8_output,layer8_intermediate,layer8_attention_output, \\\n",
    "        #                       layer9_output,layer9_intermediate,layer9_attention_output, \\\n",
    "        #                       layer10_output,layer10_intermediate,layer10_attention_output, \\\n",
    "        #                       layer11_output,layer11_intermediate,layer11_attention_output,fc\"\n",
    "        # Which layer neurons to describe. String list of layer names to describe, separated by comma (no spaces). \n",
    "        # Follows the naming format of the Pytorch module used.\n",
    "\n",
    "        # self.target_layers = \"layer0_1,layer0_2,layer1_1,layer1_2,layer2_1,layer2_2,layer3_1,layer3_2,layer4_1,layer4_2,layer5_1,layer5_2,layer6_1,layer6_2,layer7_1,layer7_2,layer8_1,layer8_2,layer9_1,layer9_2,layer10_1,layer10_2,layer11_1,layer11_2,fc\"\n",
    "        \n",
    "        self.probing_dataset = \"esc50\"  # Probing dataset to probe the target model\n",
    "        self.concept_set_file = \"data/concept_set/esc50.txt\"  # Path to txt file of concept set\n",
    "        self.network_class_file = \"data/network_class/esc50.txt\"  # Path to txt file of network's classification class\n",
    "        self.clip_model = \"ViT-B/32\"  # CLIP model version to use\n",
    "        self.clap_model = \"ViT-B/32\"  # CLAP model version to use\n",
    "        self.sentence_transformer = 'all-MiniLM-L12-v2'  # Sentence transformer to use\n",
    "        self.batch_size = 1  # Batch size when running CLIP/target model\n",
    "        self.device = \"cuda\"  # Whether to use GPU/which GPU\n",
    "        self.seed = 20  # Seed number\n",
    "        self.num_of_gpus = 1  # Number of available GPUs for vllm\n",
    "        self.pool_mode = \"avg\"  # Aggregation function for channels\n",
    "        self.scoring_func = False  # Scoring function flag\n",
    "\n",
    "        # Directory paths\n",
    "        self.audio_description_dir = \"audio_description\"  # Directory to save audio descriptions\n",
    "        self.audio_dir = \"save_audios\"  # Directory to save audio\n",
    "        self.save_activation_dir = \"saved_activations\"  # Directory to save activation values\n",
    "        self.save_summary_dir = \"summaries\"  # Directory to save summaries\n",
    "        self.save_discriminative_sample_dir = \"discriminative_samples\"  # Directory to save discriminative samples\n",
    "        self.save_prediction_dir = \"prediction\"  # Directory to save prediction\n",
    "        self.save_interpretability_dir = 'interpretability'  # Directory to save interpretability experiments\n",
    "\n",
    "        # Discriminative settings\n",
    "        self.discriminative_type = \"highly\"  # Type of discriminative samples\n",
    "        self.post_process_type = \"sim\"  # Post-processing type\n",
    "        self.mutual_info_threshold = 0.6  # Mutual information threshold\n",
    "        self.K = 5  # Top-K highly/lowly-activated audio\n",
    "        self.clusters = 11  # Number of clusters\n",
    "\n",
    "        # LLM settings\n",
    "        self.llm = \"meta-llama/Llama-2-13b-chat-hf\"  # LLM to use\n",
    "        self.top_p = 1.0  # Sampling parameter: top-p\n",
    "        self.temperature = 1.0  # Sampling parameter: temperature\n",
    "        self.max_tokens = 128  # Sampling parameter: max tokens\n",
    "        self.ICL_topk = 1  # Experiments of top5 or top1 accuracy of ICL\n",
    "\n",
    "        # Pruning settings\n",
    "        self.save_pruning_dir = \"pruning_result\"  # Directory to save pruning results\n",
    "        self.max_pruned_num = 3000  # Maximum number of pruned neurons\n",
    "        self.pruned_concepts = [\"water_drops\"]  # Concepts to be ablated\n",
    "        self.pruning_strategy = \"db\"  # Method to decide pruned neurons (random, db, tab, ocp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang1234/env/dissect/lib/python3.8/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/yuxiang1234/env/dissect/lib/python3.8/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load our best checkpoint in the paper.\n",
      "The checkpoint is already downloaded\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset parquet (/work/yuxiang1234/cache/ashraq___parquet/ashraq--esc50-1000c3b73cc1500f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcba04e3c9d04645a47e86fee262afa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /work/yuxiang1234/cache/ashraq___parquet/ashraq--esc50-1000c3b73cc1500f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-6c98728cad801bf4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exist, load saved_activations/ast-esc50_esc50_esc50/text_esc50.pt\n",
      "file exist, load saved_activations/ast-esc50_esc50_esc50/text_esc50.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset parquet (/work/yuxiang1234/cache/ashraq___parquet/ashraq--esc50-1000c3b73cc1500f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9955fcb024457185c8bf57e183e9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target layer: fc\n"
     ]
    }
   ],
   "source": [
    "args.save_activation_dir = os.path.join(args.save_activation_dir, f\"{args.target_name}_{args.probing_dataset}_{get_basename(args.concept_set_file)}\")\n",
    "\n",
    "save_activations(args)\n",
    "\n",
    "args.target_layers = args.target_layers.split(\",\")\n",
    "\n",
    "dataset = save_discriminative_sample(args.save_discriminative_sample_dir, args.save_activation_dir, args.probing_dataset, args.concept_set_file, args.target_name, args.target_layers, args.K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed-concept Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity function:  cos-similarity\n",
      "Calculate accuracy on 50 neurons out of 50 neurons\n",
      "cos-similarity Top-1 fc acc:80.0000\n",
      "cos-similarity Top-5 fc acc:82.0000\n",
      "DB cos-similarity fc cos sim: 0.8487\n",
      "similarity function:  cos_similarity_cubed\n",
      "Calculate accuracy on 50 neurons out of 50 neurons\n",
      "cos_similarity_cubed Top-1 fc acc:100.0000\n",
      "cos_similarity_cubed Top-5 fc acc:100.0000\n",
      "DB cos_similarity_cubed fc cos sim: 1.0000\n",
      "similarity function:  rank_reorder\n",
      "Calculate accuracy on 50 neurons out of 50 neurons\n",
      "rank_reorder Top-1 fc acc:86.0000\n",
      "rank_reorder Top-5 fc acc:100.0000\n",
      "DB rank_reorder fc cos sim: 0.9331\n",
      "similarity function:  wpmi\n",
      "Calculate accuracy on 50 neurons out of 50 neurons\n",
      "wpmi Top-1 fc acc:88.0000\n",
      "wpmi Top-5 fc acc:100.0000\n",
      "DB wpmi fc cos sim: 0.9466\n",
      "similarity function:  soft_wpmi\n",
      "Calculate accuracy on 50 neurons out of 50 neurons\n",
      "soft_wpmi Top-1 fc acc:2.0000\n",
      "soft_wpmi Top-5 fc acc:10.0000\n",
      "DB soft_wpmi fc cos sim: 0.2356\n"
     ]
    }
   ],
   "source": [
    "clip_model, clip_preprocess = clip.load(args.clip_model, device=args.device)\n",
    "transformer_model = SentenceTransformer(args.sentence_transformer)\n",
    "\n",
    "concepts = get_concept_set(args.concept_set_file)\n",
    "descriptions = get_audio_description(args.audio_description_dir, args.probing_dataset)\n",
    "\n",
    "cls_id_to_label = get_cls_id_to_label(args.network_class_file)\n",
    "\n",
    "discriminative_sample_file = os.path.join(args.save_discriminative_sample_dir, f\"{args.target_name}_{args.probing_dataset}_{get_basename(args.concept_set_file)}.json\")\n",
    "neuron_ordered_activation = read_json(discriminative_sample_file)\n",
    "\n",
    "# similarity_names = [\"cos_similarity_cubed\"]\n",
    "# similarity_fns = [similarity.cos_similarity_cubed]\n",
    "similarity_names = [\"cos-similarity\", \"cos_similarity_cubed\", \"rank_reorder\", \"wpmi\", \"soft_wpmi\"]\n",
    "similarity_fns = [similarity.cos_similarity, similarity.cos_similarity_cubed, similarity.rank_reorder, similarity.wpmi, similarity.soft_wpmi]\n",
    "\n",
    "target_layers = args.target_layers\n",
    "results = defaultdict(lambda: defaultdict(dict))\n",
    "args.save_dir = os.path.join(args.save_activation_dir, f'{args.target_name}_{args.probing_dataset}_{get_basename(args.concept_set_file)}')\n",
    "\n",
    "with open(args.concept_set_file, 'r') as f: \n",
    "\tconcept_features = []\n",
    "\tbs = 100\n",
    "\tfor step in range(0, len(concepts) // bs + 1):\n",
    "\t\tcon = concepts[bs*step: bs*(step+1)]\n",
    "\t\tword = clip.tokenize(con, truncate=True).to(args.device)\n",
    "\t\tword_feature = clip_model.encode_text(word).detach().cpu()\n",
    "\t\tconcept_features.append(word_feature)\n",
    "\tconcept_features = torch.cat(concept_features, 0).to(args.device).float()\n",
    "\n",
    "for target_layer in target_layers:\n",
    "\tfor similarity_fn, similarity_name in zip(similarity_fns, similarity_names):\n",
    "\t\t\n",
    "\t\tsimilarities, keys = get_similarity_from_descriptions(target_layer, clip_model, \n",
    "\t\t\tneuron_ordered_activation, descriptions, concept_features, similarity_fn, device=args.device)\n",
    "\n",
    "\t\tprint('similarity function: ',similarity_name)\n",
    "\t\tif target_layer == 'fc':\n",
    "\t\t\tnum_of_neuron = len([value for value in cls_id_to_label.values() if value != None])\n",
    "\t\t\tprint(f\"Calculate accuracy on {num_of_neuron} neurons out of {len(cls_id_to_label)} neurons\")\n",
    "\n",
    "\t\t\tprint(f\"{similarity_name} Top-1 fc acc:{get_topk_acc(similarities, cls_id_to_label, concepts, k=1):.4f}\")\n",
    "\t\t\tprint(f\"{similarity_name} Top-5 fc acc:{get_topk_acc(similarities, cls_id_to_label, concepts, k=5):.4f}\")\n",
    "\n",
    "\t\t\tpred = [concepts[int(p)] for p in torch.argmax(similarities, dim=1)]\n",
    "\n",
    "\t\t\tgt_embeds = transformer_model.encode([cls_id_to_label[i] for i in range(len(cls_id_to_label))])\n",
    "\t\t\tpred_embeds = transformer_model.encode(pred)\n",
    "\t\t\tcos_sim_mpnet = np.sum(pred_embeds * gt_embeds, axis=1)\n",
    "\n",
    "\t\t\tcos_sim = float(np.mean(cos_sim_mpnet))\n",
    "\t\t\tprint(\"DB {} fc cos sim: {:.4f}\".format(similarity_name, cos_sim))\n",
    "\n",
    "\t\tprediction, gt = get_clip_prediction(similarities, cls_id_to_label, concepts, K=args.K, final_layer=(target_layer=='fc'))\n",
    "\n",
    "\t\tfor i in range(len(similarities)):\n",
    "\t\t\tresults[keys[i]][similarity_name] = {'gt': gt[i], f'prediction': prediction[i]} if target_layer == 'fc' else {'gt': None, 'prediction': prediction[i]}\n",
    "\n",
    "if not os.path.exists(args.save_prediction_dir):\n",
    "\tos.makedirs(args.save_prediction_dir)\n",
    "\n",
    "with open(os.path.join(args.save_prediction_dir, f\"db-{args.target_name}-top{args.K}.json\"), \"w\") as f: \n",
    "\tjson.dump(results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.ICL_topk == 1:\n",
    "\tprompt_template =  \"<s>[INST] <<SYS>>\\n\\\n",
    "\tYou have a set of object classnames: \\\n",
    "\t{} \\\n",
    "\t\\n \\\n",
    "\tThe following is a description about some audio clips. Based on the description, select a classname out of the above classnames that matches the description most. \\\n",
    "\t<</SYS>>\\n\\n \\\n",
    "\tThe audio features a car meowing: All of the clips contain the sound of a cat meowing. Loud sound: These clips are all of loud sound but with varying degrees of intensity. Repetitive barking: Clips 1 and 4 are repetitive, with the cat meowing multiple times in each clip. Poor audio quality: All clips have poor audio quality, with either distortion, muffling, or apparent background noises. \\\n",
    "\t[/INST] \\\n",
    "\tWe know these clips are about the class 'cat' in the concept set. We can get this answer since the description mentions All of the clips contain the sound of a cat meowing'. \\n \\\n",
    "\tAnswer: cat \\\n",
    "\t</s><s>[INST] \\\n",
    "\tThey all feature a person snoring loudly. The snoring starts off slow and gets louder over time. The audio is recorded in mono. There are no other sounds in the background. The snoring is described as loud and intense. The audio clips differ in the following ways. The first clip features a man snoring, while the second and fourth clips feature a person snoring (gender not specified). The third clip features a zombie growling and snarling, while the other clips only feature snoring. The third clip is described as scary and creepy, while the other clips are not. The third clip is intended for use in a horror movie or zombie video game, while the other clips do not have specific intended uses stated. The third clip is of poor quality, while the other clips are not specified as such. \\\n",
    "\t[/INST] \\\n",
    "\tBased on the description, the most suitable classname for the audio clips would be 'snoring' or 'zombie_growling_and_snarling'. Both of these classnames match the description of loud sounds with a strong emotional impact, specifically fear and terror. But 'zombie_growling_and_snarling' is not in the given classname set. So the answer is 'snoring' \\\n",
    "\tAnswer: soring \\\n",
    "\t</s><s>[INST] \\\n",
    "\t{} \\\n",
    "\t[/INST]\"\n",
    "else:\n",
    "\tprompt_template =  \"<s>[INST] <<SYS>>\\n\\\n",
    "\tYou have a set of object classnames: \\\n",
    "\t{} \\\n",
    "\t\\n \\\n",
    "\tThe following is a description about some audio clips. Based on the description, select 5 classnames out of the above classnames that matches the description most and given the reason. There should be exactly 5 answers.  \\\n",
    "\t<</SYS>>\\n\\n \\\n",
    "\tThe audio features a cat meowing: All of the clips contain the sound of a cat meowing. Loud sound: These clips are all of loud sound but with varying degrees of intensity. Repetitive barking: Clips 1 and 4 are repetitive, with the cat meowing multiple times in each clip. Poor audio quality: All clips have poor audio quality, with either distortion, muffling, or apparent background noises. \\\n",
    "\t[/INST] \\\n",
    "\tWe know these clips are about the class 'cat' in the concept set. We can get this answer since the description mentions All of the clips contain the sound of a cat meowing'. \\n \\\n",
    "\tAnswer: cat, sheep, snoring, cow, coughing \\\n",
    "\t</s><s>[INST] \\\n",
    "\tThey all feature a person snoring loudly. The snoring starts off slow and gets louder over time. The audio is recorded in mono. There are no other sounds in the background. The snoring is described as loud and intense. The audio clips differ in the following ways. The first clip features a man snoring, while the second and fourth clips feature a person snoring (gender not specified). The third clip features a zombie growling and snarling, while the other clips only feature snoring. The third clip is described as scary and creepy, while the other clips are not. The third clip is intended for use in a horror movie or zombie video game, while the other clips do not have specific intended uses stated. The third clip is of poor quality, while the other clips are not specified as such. \\\n",
    "\t[/INST] \\\n",
    "\tBased on the description, the most suitable classname for the audio clips would be 'snoring' or 'zombie_growling_and_snarling'. Both of these classnames match the description of loud sounds with a strong emotional impact, specifically fear and terror. But 'zombie_growling_and_snarling' is not in the given classname set. So the answer is 'snoring' \\\n",
    "\tAnswer: snoring, zombie_growling_and_snarling, breathing, footsteps, coughing\\\n",
    "\t</s><s>[INST] \\\n",
    "\t{} \\\n",
    "\t[/INST]\"\n",
    "\n",
    "dataset = get_concept_dataset(args.save_summary_dir, args.probing_dataset, args.concept_set_file, args.target_name, args.target_layers, args.network_class_file, prompt_template, K=args.K)\n",
    "\n",
    "llm = LLM(model=args.llm, gpu_memory_utilization=0.9)\n",
    "sampling_params = SamplingParams(top_p=args.top_p, temperature=args.temperature, max_tokens=512)\n",
    "\n",
    "results = []\n",
    "inputs = [line[\"text\"] for line in dataset]\n",
    "outputs = llm.generate(inputs, sampling_params)\n",
    "\n",
    "for inp, out in zip(dataset, outputs):\n",
    "\ttarget_layer = inp[\"target_layer\"]\n",
    "\tneuron_id = inp[\"neuron_id\"]\n",
    "\tneuron_label = inp[\"neuron_label\"]\n",
    "\tprint(f'-------- {target_layer} # {neuron_id}  --------')\n",
    "\n",
    "\tresponse = out.outputs[0].text\n",
    "\tresults.append({\"response\": response,\n",
    "\t\t\t\t\t\"target_layer\": target_layer,\n",
    "\t\t\t\t\t\"neuron_id\": neuron_id,\n",
    "\t\t\t\t\t\"neuron_label\": neuron_label})\n",
    "\n",
    "\tif target_layer == \"fc\":\n",
    "\t\tprint(\"gt label: \", neuron_label)\n",
    "\t\tprint(response)\n",
    "\n",
    "\tprint(\"=\" * 20)\n",
    "\n",
    "# calculate accuracies\n",
    "concepts = get_concept_set(args.concept_set_file)\n",
    "\n",
    "if not os.path.exists(args.save_prediction_dir):\n",
    "\tos.makedirs(args.save_prediction_dir)\n",
    "\n",
    "is_correct, prediction, cos_sim = [], [], []\n",
    "for line in results:\n",
    "\tprediction, sim = post_process_prediction(line[\"response\"], concepts=concepts, gt=line[\"neuron_label\"], embedding_model=args.sentence_transformer, K=args.ICL_topk)\n",
    "\t\n",
    "\tis_correct.append(1) if line[\"neuron_label\"] in prediction else is_correct.append(0)\n",
    "\tcos_sim.append(sim)\n",
    "\n",
    "print(\"accuracy: \", round(sum(is_correct) / len(is_correct) * 100, 2))\n",
    "print(\"cos sim : \", round(sum(cos_sim) / len(cos_sim), 2))\n",
    "\n",
    "# save prediction\n",
    "prediction_file = os.path.join(args.save_prediction_dir, f'{args.target_name}_{args.probing_dataset}_{args.concept_set_file.split(\"/\")[-1].split(\".txt\")[0]}_{args.K}_{args.ICL_topk}.json')\n",
    "\n",
    "with open(prediction_file, \"w\") as f:\n",
    "\tjson.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos-similarity\n",
      "Calculate accuracy on 50 neurons out of 50 neurons\n",
      "TAB Top 1 acc:2.0000\n",
      "TAB Top 5 acc:16.0000\n",
      "TAB cos sim   :0.2855\n",
      "cos_similarity_cubed\n",
      "Calculate accuracy on 50 neurons out of 50 neurons\n",
      "TAB Top 1 acc:92.0000\n",
      "TAB Top 5 acc:98.0000\n",
      "TAB cos sim   :0.7838\n",
      "rank_reorder\n",
      "Calculate accuracy on 50 neurons out of 50 neurons\n",
      "TAB Top 1 acc:84.0000\n",
      "TAB Top 5 acc:92.0000\n",
      "TAB cos sim   :0.7789\n",
      "wpmi\n",
      "Calculate accuracy on 50 neurons out of 50 neurons\n",
      "TAB Top 1 acc:96.0000\n",
      "TAB Top 5 acc:100.0000\n",
      "TAB cos sim   :0.7932\n",
      "soft_wpmi\n",
      "Calculate accuracy on 50 neurons out of 50 neurons\n",
      "TAB Top 1 acc:96.0000\n",
      "TAB Top 5 acc:98.0000\n",
      "TAB cos sim   :0.7343\n"
     ]
    }
   ],
   "source": [
    "transformer_model = SentenceTransformer(args.sentence_transformer)\n",
    "\n",
    "concepts = get_concept_set(args.concept_set_file)\n",
    "cls_id_to_label = get_cls_id_to_label(args.network_class_file)\n",
    "\n",
    "similarity_names = [\"cos-similarity\", \"cos_similarity_cubed\", \"rank_reorder\", \"wpmi\", \"soft_wpmi\"]\n",
    "similarity_fns = [similarity.cos_similarity, similarity.cos_similarity_cubed, similarity.rank_reorder, similarity.wpmi, similarity.soft_wpmi]\n",
    "\n",
    "results = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for target_layer in target_layers:\n",
    "\ttarget_save_name = f\"{args.save_activation_dir}/target_{args.probing_dataset}_{args.target_name}_{target_layer}.pt\"\n",
    "\taudio_save_name = f\"{args.save_activation_dir}/audio_{args.probing_dataset}.pt\"\n",
    "\ttext_save_name = f\"{args.save_activation_dir}/text_{args.concept_set_file.split('/')[-1].replace('.txt', '')}.pt\"\n",
    "\n",
    "\tfor similarity_fn, similarity_name in zip(similarity_fns, similarity_names):\n",
    "\n",
    "\t\tsimilarities, target_feats = get_similarity_from_activations(target_save_name,   \n",
    "\t\t\taudio_save_name, text_save_name, similarity_fn, device=args.device)\n",
    "\t\t\n",
    "\t\tif target_layer == \"fc\":\n",
    "\t\t\tprint(similarity_name)\n",
    "\t\t\tnum_of_neuron = len([value for value in cls_id_to_label.values() if value != None])\n",
    "\t\t\tprint(f\"Calculate accuracy on {num_of_neuron} neurons out of {len(cls_id_to_label)} neurons\")\n",
    "\t\t\tconcepts = get_concept_set(args.concept_set_file, clip_format=False)\n",
    "\n",
    "\t\t\tprint(f\"TAB Top 1 acc:{get_topk_acc(similarities, cls_id_to_label, concepts, k=1):.4f}\")\n",
    "\t\t\tprint(f\"TAB Top 5 acc:{get_topk_acc(similarities, cls_id_to_label, concepts, k=5):.4f}\")\n",
    "\n",
    "\n",
    "\t\t\tprediction, gt = get_clip_prediction(similarities, cls_id_to_label, concepts, K=args.K, final_layer=True)\n",
    "\n",
    "\t\t\t# pred = torch.argmax(similarities, dim=1)\n",
    "\t\t\t# pred = [concepts[int(p)] for p in pred]\n",
    "\t\t\t\n",
    "\t\t\tgt_embeds = transformer_model.encode([cls_id_to_label[i] for i in range(len(cls_id_to_label))])\n",
    "\t\t\tpred_embeds = transformer_model.encode(prediction)\n",
    "\t\t\tcos_sim_mpnet = np.sum(pred_embeds * gt_embeds, axis=1)\n",
    "\n",
    "\t\t\tcos_sim = float(np.mean(cos_sim_mpnet))\n",
    "\t\t\tprint(\"TAB cos sim   :{:.4f}\".format(cos_sim))\n",
    "\n",
    "\t\t\tfor i in range(len(similarities)):\n",
    "\t\t\t\tkey = f\"{target_layer}#{i}\"\n",
    "\t\t\t\tresults[key][similarity_name] = {'gt': gt[i], f'prediction': prediction[i]}\n",
    "\t\telse:\n",
    "\t\t\tprediction, _ = get_clip_prediction(similarities, cls_id_to_label, concepts, K=args.K, final_layer=False)\n",
    "\n",
    "\t\t\tfor i in range(len(similarities)):\n",
    "\t\t\t\tkey = f\"{target_layer}#{i}\"\n",
    "\t\t\t\tresults[key][similarity_name] = {'gt': None, f'prediction': prediction[i]}\n",
    "\n",
    "if os.path.exists(args.save_prediction_dir) == False:\n",
    "\tos.makedirs(args.save_prediction_dir)\n",
    "\n",
    "with open(os.path.join(args.save_prediction_dir, f\"tab-{args.target_name}-top{args.K}.json\"), \"w\") as f: \n",
    "\tjson.dump(results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
