{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this code extract POS from the summary json file, which generates a new json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from vllm import LLM, SamplingParams\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dummy(adjs):\n",
    "    dummy = set(['other', 'most', 'some', 'any', 'several', 'many', 'few', 'all', 'each', 'every', 'another', 'both', 'either', 'neither', 'such', 'more', 'less', 'a few', 'a lot', 'several', 'many', 'much', 'little', 'most', 'none', 'no one', 'somebody', 'someone', 'something', 'somewhere', 'used', 'audio', 'best', 'due', 'recorded', 'most', 'various', 'video', 'meant', 'easy', '737-800', 'personal', 'external', 'overall',\n",
    "    'sound', 'mobile', 'designed', 'well-defined', 'detailed', 'suitable', 'small', 'third', 'second', 'fourth', 'fifth', 'first', 'related', 'different', 'actual', 'kitchen', '*', '2-3', 'everyday', 'common'])\n",
    "    return adjs.difference(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up paramters and in/out file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 10\n",
    "target_name = 'ast-esc50'\n",
    "# target_name = 'beats-esc50'\n",
    "# target_name = 'beats-esc50-unfreeze'\n",
    "\n",
    "in_json = f'summaries/calibration_{target_name}_esc50_esc50_top5.json'\n",
    "out_json = f'summaries/calibration_{target_name}_esc50_esc50_top5_processed.json'\n",
    "\n",
    "with open(in_json, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract all adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adjs, new_data = [], {}\n",
    "for k, item in tqdm(data.items()):\n",
    "    sentence = ''\n",
    "    for point in item['highly']:\n",
    "        sentence = sentence + ' ' + point\n",
    "\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    # rule-based filtering\n",
    "    adjs = []\n",
    "    for i in range(len(pos_tags)):\n",
    "        if pos_tags[i][1] in ['JJ', 'JJR', 'JJS', 'VBN']:\n",
    "            previous = pos_tags[i-4:i]\n",
    "            previous = [k[0] for k in previous]\n",
    "            if 'no' not in previous and 'not' not in previous:\n",
    "                adjs.append(pos_tags[i][0])\n",
    "\n",
    "    adjs = set([a for a in adjs])\n",
    "    adjs = list(remove_dummy(adjs))\n",
    "    item['adj_after_rbf'] = adjs\n",
    "    new_data.update({k: item})\n",
    "    all_adjs.extend(adjs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply the LLM to determine which are acoustic adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adjs = list(set(all_adjs))\n",
    "prompt_template =  \"<s>[INST] <<SYS>>\\n\\\n",
    "<</SYS>>\\n\\n \\\n",
    "Can the adjective '{}' be used to describe the tone, emotion, or acoustic features of audio, music, or any other form of sound?\\n \\\n",
    "Answer(yes or no):\\n\\\n",
    "Reason:\\\n",
    "[/INST]\"\n",
    "\n",
    "dataset = defaultdict(list)\n",
    "for word in all_adjs:\n",
    "\tdataset[\"word\"].append(word)\n",
    "\tdataset[\"text\"].append(prompt_template.format(word))\n",
    "dataset = Dataset.from_dict(dataset)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=16, shuffle=False, pin_memory=True,num_workers=16)\n",
    "\n",
    "# load a hf LLM. Change to your path.\n",
    "llm = LLM(model=\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "sampling_params = SamplingParams(top_p=1, temperature=1, max_tokens=128)\n",
    "\n",
    "prompts = [prompt_template.format(w) for w in all_adjs]\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "results, ref_dict = [], {}\n",
    "for idx, output in enumerate(outputs):\n",
    "\tprompt = output.prompt\n",
    "\tword = dataset[idx][\"word\"]\n",
    "\tgenerated_text = output.outputs[0].text\n",
    "\t# print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "\tresults.append({\"word\": word, \"response\": generated_text})\n",
    "\n",
    "\tans = True if 'yes' in generated_text.lower() else False\n",
    "\tref_dict[word] = ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine filtered results to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_data = {}\n",
    "for k, item in tqdm(new_data.items()):\n",
    "    adjs = item['adj_after_rbf']\n",
    "    valid_adjs = []\n",
    "    for adj in adjs:\n",
    "        adj = adj.strip('\\'')\n",
    "        if adj in ref_dict.keys() and ref_dict[adj]:\n",
    "            valid_adjs.append(adj)\n",
    "\n",
    "    item['adj_after_rbf_llmf'] = valid_adjs\n",
    "    new_new_data.update({k: item})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_new_data = {}\n",
    "for k, item in tqdm(new_new_data.items()):\n",
    "    sentence = ''\n",
    "    for point in item['highly']:\n",
    "        sentence = sentence + ' ' + point\n",
    "    doc = nlp(sentence.strip())\n",
    "    caption_len = len([token.text for token in doc])\n",
    "\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    verbs, preps, nouns = [], [], []\n",
    "    for pos_tag in pos_tags:\n",
    "        if pos_tag[1] in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']: # verb\n",
    "            if pos_tag[0] not in stopwords.words('english'):\n",
    "                verbs.append(pos_tag[0])\n",
    "        if pos_tag[1] in ['IN']: # preposition. Do not use stopwords to filter\n",
    "            preps.append(pos_tag[0])\n",
    "        if pos_tag[1] in ['NN', 'NNS', 'NNP', 'NNPS']: # noun\n",
    "            if pos_tag[0] not in stopwords.words('english'):\n",
    "                nouns.append(pos_tag[0])\n",
    "\n",
    "    verbs =list(set([a for a in verbs]))\n",
    "    preps = list(set([a for a in preps]))\n",
    "    nouns = list(set([a for a in nouns]))\n",
    "    item['verbs'] = verbs\n",
    "    item['preps'] = preps\n",
    "    item['nouns'] = nouns\n",
    "    item['caption_len'] = caption_len\n",
    "    new_new_new_data.update({k: item})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dump processed data to out file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_json, 'w') as f: \n",
    "\tjson.dump(new_new_new_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot adjective distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = defaultdict(int)\n",
    "for line in new_new_new_data.values(): \n",
    "\tfor w in line['adj_after_rbf_llmf']:\n",
    "\t\tcounter[w] += 1\n",
    "\n",
    "# fix empirical error\n",
    "counter['high-quality'] += counter['high']\n",
    "del counter['high']\n",
    "counter['low-quality'] += counter['low']\n",
    "del counter['low']\n",
    "\n",
    "data = {k: v for k, v in sorted(counter.items(), key=lambda x: x[1], reverse=True)}\n",
    "x = list(data.keys())[:topk]\n",
    "x.reverse()\n",
    "y = list(data.values())[:topk]\n",
    "y.reverse()\n",
    "\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(y)))\n",
    "plt.figure(figsize=(25, 32))\n",
    "plt.barh(x, y, color=colors, edgecolor='none')\n",
    "plt.xticks(fontsize=50, rotation=30, ha='right') \n",
    "plt.yticks(fontsize=60)\n",
    "plt.subplots_adjust(left=0.25, right=0.95, top=0.98)\n",
    "plt.show()\n",
    "plt.savefig(f'adjective_count-{target_name}.jpg', format='jpg', dpi=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
